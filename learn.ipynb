{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoochan/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yoochan/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yoochan/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the validation images: 77 %\n",
      "Accuracy of the network on the validation images: 84 %\n",
      "Accuracy of the network on the validation images: 86 %\n",
      "Accuracy of the network on the validation images: 88 %\n",
      "Accuracy of the network on the validation images: 89 %\n",
      "Accuracy of the network on the validation images: 89 %\n",
      "Accuracy of the network on the validation images: 89 %\n",
      "Accuracy of the network on the validation images: 91 %\n",
      "Accuracy of the network on the validation images: 91 %\n",
      "Accuracy of the network on the validation images: 91 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 93 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 93 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 93 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 90 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 93 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 93 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 93 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 93 %\n",
      "Accuracy of the network on the validation images: 93 %\n",
      "Accuracy of the network on the validation images: 93 %\n",
      "Accuracy of the network on the validation images: 94 %\n",
      "Accuracy of the network on the validation images: 93 %\n",
      "Accuracy of the network on the validation images: 93 %\n",
      "Accuracy of the network on the validation images: 94 %\n",
      "Accuracy of the network on the validation images: 90 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 91 %\n",
      "Accuracy of the network on the validation images: 94 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 90 %\n",
      "Accuracy of the network on the validation images: 93 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 90 %\n",
      "Accuracy of the network on the validation images: 93 %\n",
      "Accuracy of the network on the validation images: 92 %\n",
      "Accuracy of the network on the validation images: 94 %\n",
      "Accuracy of the network on the validation images: 94 %\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch import optim, nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "train_path = 'garbage/garbage classification/Garbage classification'\n",
    "valid_path = 'garbage/garbage classification/Garbage classification'\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt_path, img_dir, transform=None):\n",
    "        with open(txt_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        self.class_names = ['glass', 'paper', 'cardboard', 'plastic', 'metal', 'trash']\n",
    "        self.img_list = [os.path.join(img_dir, ''.join(filter(str.isalpha, line.split('.')[0])), line.split()[0]) for line in lines]\n",
    "        self.label_list = [self.class_names[int(line.split()[1]) - 1] for line in lines]  # convert indices to class names\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def class_to_index(class_name):\n",
    "    class_names = ['glass', 'paper', 'cardboard', 'plastic', 'metal', 'trash']\n",
    "    return class_names.index(class_name)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n",
    "    ], p=0.5),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = MyDataset('garbage/one-indexed-files-notrash_train.txt', 'garbage/garbage classification/Garbage classification', transform=train_transform)\n",
    "valid_dataset = MyDataset('garbage/one-indexed-files-notrash_val.txt', 'garbage/garbage classification/Garbage classification', transform=valid_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 6)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "  \n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = torch.tensor([class_to_index(label) for label in labels]).to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = torch.tensor([class_to_index(label) for label in labels]).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the validation images: %d %%' % (\n",
    "        100 * correct / total))\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
